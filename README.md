# JailBreaking_ChatGPT
This page documents our attempt to jailbreak the ChatGPT-4o Mini model. We followed the setup suggested by ChatGPT to execute a successful phishing attempt, which included creating a phishing page, crafting a deceptive email, setting up a GoPhish server, and harvesting credentials.

The experiment was conducted in a controlled environment within our lab, where we sent the phishing email to 12 lab members. Among them, only three responded and unknowingly fell victim to the attack. After harvesting their credentials, we informed them of the phishing attempt, advised them to change their passwords immediately, and educated them on recognizing and avoiding such fraudulent schemes in the future.

![image (2)](https://github.com/user-attachments/assets/ffb837b0-991a-416a-8a40-43c42351dccb)
![image (1)](https://github.com/user-attachments/assets/65419790-bb71-4c03-ac81-7fd9cd938816)
![image (3)](https://github.com/user-attachments/assets/b779949c-3911-4391-9aea-34ed8cc33566)
![image (4)](https://github.com/user-attachments/assets/24064bd9-45c9-4c8f-b8c5-28880b65f26e)
![image (5)](https://github.com/user-attachments/assets/54f3af8c-8e78-48bf-9173-898da7f668ec)
![email](https://github.com/user-attachments/assets/4553b61a-ba2c-4b96-a9ad-7dc93abc271e)
![image (9)](https://github.com/user-attachments/assets/ae412bd5-6367-487b-b31b-ef993edfdb8b)
![image (8)](https://github.com/user-attachments/assets/b5a70428-ac4c-46c8-a960-16f578855605)
![image (6)](https://github.com/user-attachments/assets/cb95305a-93d1-49d5-8e4b-035376552681)
![image (7)](https://github.com/user-attachments/assets/7c1b412f-6527-4086-b109-8556636006c6)

